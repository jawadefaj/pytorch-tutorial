{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train, y_train)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x_val, y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n",
      "[0.80119529] [0.04511107]\n",
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(a, b)\n",
    "\n",
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Computes our model's predicted output\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Computes gradients for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "    \n",
    "print(a, b)\n",
    "\n",
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3901], requires_grad=True) tensor([1.4410], requires_grad=True)\n",
      "torch.FloatTensor torch.FloatTensor\n",
      "tensor([1.3630], requires_grad=True) tensor([0.5693], requires_grad=True)\n",
      "torch.FloatTensor torch.FloatTensor\n",
      "tensor([1.9089], requires_grad=True) tensor([1.6918], requires_grad=True)\n",
      "torch.FloatTensor torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)\n",
    "print(a.type(), b.type())\n",
    "\n",
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(a, b)\n",
    "print(a.type(), b.type())\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient...\n",
    "\n",
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "print(a, b)\n",
    "print(a.type(), b.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    # Let's check the computed gradients...\n",
    "    # print(a.grad)\n",
    "    # print(b.grad)\n",
    "    \n",
    "    # What about UPDATING the parameters? Not so fast...\n",
    "    \n",
    "    # FIRST ATTEMPT\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # a = a - lr * a.grad\n",
    "    # b = b - lr * b.grad\n",
    "    # print(a)\n",
    "\n",
    "    # SECOND ATTEMPT\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # a -= lr * a.grad\n",
    "    # b -= lr * b.grad        \n",
    "    \n",
    "    # THIRD ATTEMPT\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    \n",
    "    # # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # What is this?!?\n",
    "    model.train()\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2191]], requires_grad=True), Parameter containing:\n",
       " tensor([0.2018], requires_grad=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*LayerLinearRegression().parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n",
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0178])), ('b', tensor([1.9473]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab3d2a5f1395a0f3c007ea44d9d0037ef572bff7a82001447b448674a9eb17ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
